---
title: "ADS 503: Cervical Cancer Biopsy Prediction Project"
author: "Ruddy Simonpour & Shailja Somani"
output: pdf_document
date: "May 30, 2023"
---

```{r, warning=FALSE}
# load necessary packages for files above
library(Hmisc)
library(dplyr)
library(pROC)

```

```{r}
setwd("/Users/ruddysimonpour/Desktop/University of Sandiego - Curriculum/ADS 503 - Applied Predictive Modeling/ADS503-Applied-Predictive-Modeling/Pipeline")   #choose a location/path and set the working directory
source ("Data_Ingestion.R")
source ("Viz_EDA.R")
source ("Preprocessing.R")
source ("Modeling.R")
```


```{r}
# Uses functions from files loaded in to clean data
set.seed(007)

# loading Data
cervical_data_raw <- read_data(x="/Users/ruddysimonpour/Desktop/University of Sandiego - Curriculum/ADS 503 - Applied Predictive Modeling/ADS503-Applied-Predictive-Modeling/input_resource/kag_risk_factors_cervical_cancer.csv")
head(cervical_data_raw,5)

dim(cervical_data_raw)

# check missing data
null_counts_raw <- check_nulls(cervical_data_raw)

# remove cols with more than 50% missing data
cervical_data_clean <- remove_cols(cervical_data_raw)

# check missing data
null_counts_clean <- check_nulls(cervical_data_clean)

dim(cervical_data_clean)
```

# EDA Analysis 

```{r, results='hide'}
# These user-defined functions are pulled from the Viz_EDA.R file.
# Look at all histograms of features collectively
hist.df(cervical_data_clean)

# Create boxplots for all features - helps visualize outliers
boxplot.df(cervical_data_clean)
```

# Data Cleaning

```{r}
library(caret)

# remove near zero variance variables
dim(cervical_data_clean)
degeneratecols <- nearZeroVar(cervical_data_clean)

length(degeneratecols) # number of cols that are degenerate distributions

cervical_data_process <- cervical_data_clean[, -degeneratecols]
dim(cervical_data_process)

# impute missing values with knn
data_clean <- impute_with_knn(cervical_data_process, k = 10)

# since knn imputation create new columns, we will exclude the new columns from our dataset
data_clean <- subset(data_clean, select = Age:Biopsy)

null_counts_clean <- check_nulls(data_clean)
data_clean
```

## Convert the class to factor variable

```{r}
# initial look at the target variable
data_clean$Biopsy<-as.factor(data_clean$Biopsy) # convert class to factor
levels(data_clean$Biopsy) <- c("No", "Yes") # names of factors

```


# Creating Train and Test Split

```{r}
# data splitting
set.seed(100)

trainIndex <- createDataPartition(data_clean$Biopsy, p = .8, list = FALSE)
trainData <- data_clean[trainIndex, ]
testData  <- data_clean[-trainIndex, ]

train_X <- trainData[ , !(names(trainData) %in% "Biopsy")]
train_y <- trainData$Biopsy

test_X <- testData[ , !(names(testData) %in% "Biopsy")]
test_y <- testData$Biopsy


############################################### Imbalance class variable


# plotting number of samples in each class - original dataset
options(scipen=10000)

train_y_df <- data.frame(Biopsy = train_y)

# Create the plot
p <- ggplot(data = train_y_df, aes(x = Biopsy, fill = Biopsy)) +
    geom_bar() +
    geom_text(stat='count', aes(label=..count..), vjust=1) +
    ggtitle("Number of samples in each class", subtitle = "Original dataset") +
    xlab("") +
    ylab("Samples") +
    scale_y_continuous(expand = c(0,0)) +
    scale_x_discrete(expand = c(0,0)) +
    theme(legend.position = "none", 
         legend.title = element_blank(),
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p 
ggsave(filename = "class_imbalance1.png", plot = p, width = 7, height = 7)



```
### Class Imbalanace (ROSE)

```{r}
library(ROSE)

set.seed(100)

rose_train <- ROSE(Biopsy ~ ., data = trainData)$data

train_X <- rose_train[ , !(names(rose_train) %in% "Biopsy")]
train_y <- rose_train$Biopsy


options(scipen=10000) 

train_y_df <- data.frame(Biopsy = train_y)

p1 <- ggplot(data = train_y_df, aes(x = Biopsy,fill = Biopsy)) +
    geom_bar()+
    geom_text(stat='count', aes(label=..count..), vjust=1) +
    ggtitle("Number of samples in each class after ROSE technique implementation", subtitle = "Original dataset") +
    xlab("")+
    ylab("Samples")+
    scale_y_continuous(expand = c(0,0))+
    scale_x_discrete(expand = c(0,0))+
    theme(legend.position = "none", 
         legend.title = element_blank(),
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank())
p1

ggsave(filename = "class_imbalance2.png", plot = p1, width = 7, height = 4)

```





## Data Pre-Processing

```{r}
preProcValues <- preProcess(train_X, 
                            method = c("center", "scale"))

train_X <- predict(preProcValues, train_X)
test_X <- predict(preProcValues, test_X)


cntrl <- trainControl(method = "cv", number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

```

# Modeling 

## Non-Linear models

### Neural Network Model

```{r}
### Neural Network Model
nnet_model <- train_nnet_model(train_X, train_y, ncol(trainData), cntrl)
nnet_model
```


```{r}
# get prediction result
testResults_nnet <- get_prediction_results(nnet_model, test_X, test_y)

# convert prediction levels to match observation
testResults_nnet$prediction <- ifelse(testResults_nnet$prediction == "1", "Yes", "No")

testResults_nnet

# confusion matrix
cm <- confusionMatrix(as.factor(testResults_nnet$prediction), as.factor(testResults_nnet$observation))
print(cm)

# neural network model result plot
plot(nnet_model)

nnet_model$finalModel

# roc/auc result
roc_nnet <- roc(testResults_nnet$observation, testResults_nnet$class_prob)
auc(roc_nnet)
plot(roc_nnet)

```

### Multivariate Adaptive Regression Splines (MARS)

```{r}
mars_model <- train_mars_model(train_X, train_y, 2:20, cntrl)
mars_model
```


```{r}
# get prediction result
testResults_mars <- get_prediction_results(mars_model, test_X, test_y)

# convert prediction levels to match observation
testResults_mars$prediction <- ifelse(testResults_mars$prediction == "1", "Yes", "No")

# confusion matrix
cm <- confusionMatrix(as.factor(testResults_mars$prediction), as.factor(testResults_mars$observation))
print(cm)

# mars model result plot
plot(mars_model)

mars_model$finalModel

# roc/auc result
roc_mars <- roc(testResults_mars$observation, testResults_mars$class_prob)
auc(roc_mars)
plot(roc_mars)
```

### Support Vector Machine (SVM) 

#### svmRadial

```{r}
svm_model <- train_svm_model(train_X, train_y, 20, cntrl)
svm_model
```

```{r}
# get prediction result
testResults_svm <- get_prediction_results(svm_model, test_X, test_y)

# convert prediction levels to match observation
testResults_svm$prediction <- ifelse(testResults_svm$prediction == "1", "Yes", "No")

# confusion matrix
cm <- confusionMatrix(as.factor(testResults_svm$prediction), as.factor(testResults_svm$observation))
print(cm)

# svm Radial result plot
plot(svm_model)

svm_model$finalModel

# roc/auc result
roc_svm <- roc(testResults_svm$observation, testResults_svm$class_prob)
auc(roc_svm)
plot(roc_svm)
```

#### svmPoly

```{r}
svm_modelPoly <- train_svm_poly(train_X, train_y, cntrl)
```


```{r}
# get prediction result
testResults_svmP <- get_prediction_results(svm_modelPoly, test_X, test_y)

# convert prediction levels to match observation
testResults_svmP$prediction <- ifelse(testResults_svmP$prediction == "1", "Yes", "No")

# confusion matrix
cm <- confusionMatrix(as.factor(testResults_svmP$prediction), as.factor(testResults_svmP$observation))
print(cm)

# svm Poly result plot
plot(svm_modelPoly)

svm_modelPoly$finalModel

# roc/auc result
roc_svmp <- roc(testResults_svmP$observation, testResults_svmP$class_prob)
auc(roc_svmp)
plot(roc_svmp)
```
### K-Nearest Neighbors

```{r}
knn_model <- knn_model_train(train_X, train_y, 1:11)
```

```{r}
# get prediction result
testResults_knn <- get_prediction_results(knn_model, test_X, test_y)

# convert prediction levels to match observation
testResults_knn$prediction <- ifelse(testResults_knn$prediction == "1", "Yes", "No")

# confusion matrix
cm <- confusionMatrix(as.factor(testResults_knn$prediction), as.factor(testResults_knn$observation))
print(cm)

# kNN result plot
plot(knn_model)

knn_model$finalModel

# roc/auc result
roc_knn <- roc(testResults_knn$observation, testResults_knn$class_prob)
auc(roc_knn)
plot(roc_knn)
```

### Random Forest Model
```{r}
# Train model
rf_model <- rf_model_train(train_X, train_y, cntrl)
```

```{r}
# get prediction result
testResults_rf <- get_prediction_results(rf_model, test_X, test_y)

# convert prediction levels to match observation
testResults_rf$prediction <- ifelse(testResults_rf$prediction == "1", "Yes", "No")

# confusion matrix
cm <- confusionMatrix(as.factor(testResults_rf$prediction), as.factor(testResults_rf$observation))
print(cm)

# RF result plot
plot(rf_model)

rf_model$finalModel

# roc/auc result
roc_rf <- roc(testResults_rf$observation, testResults_rf$class_prob)
auc(roc_rf)
plot(roc_rf)
```
## Linear Model

### Logistic Regression

```{r}
# build new control
lr_model <- lr_model_train(train_X, train_y, cntrl)
```

```{r}
# Test new model
# get prediction result
testResults_lr <- get_prediction_results(lr_model, test_X, test_y)

# convert prediction levels to match observation
testResults_lr$prediction <- ifelse(testResults_lr$prediction == "1", "Yes", "No")

# confusion matrix
cm <- confusionMatrix(as.factor(testResults_lr$prediction), as.factor(testResults_lr$observation))
print(cm)

# roc/auc result
roc_lr <- roc(testResults_lr$observation, testResults_lr$class_prob)
auc(roc_lr)
plot(roc_lr)
```


# Judging model benchmark using ROC curve 

```{r}
### Compare Models using ROC curve

plot(roc_nnet, type = "s", col = 'red', legacy.axes = TRUE)
plot(roc_mars, type = "s", add = TRUE, col = 'green', legacy.axes = TRUE)
plot(roc_svm, type = "s", add = TRUE, col = 'blue', legacy.axes = TRUE)
plot(roc_svmp, type = "s", add = TRUE, col = 'brown', legacy.axes = TRUE)
plot(roc_knn, type = "s", add = TRUE, col = 'yellow', legacy.axes = TRUE)
plot(roc_rf, type = "s", add = TRUE, col = 'orange', legacy.axes = TRUE)
plot(roc_lr, type = "s", add = TRUE, legacy.axes = TRUE)


legend("bottomright", legend=c("Neural Network", "Multivariate Adaptive Regression Splines (MARS)", "Support Vector Machine (svmRadial)", "Support Vector Machine (svmPoly)","K-Nearest Neighbor", "Random Forest", "Logistic Regression (Linear model)"),
       
       col=c("red", "green","blue", "brown", "yellow", "orange", "black" ), lwd=2)

title(main = "Compare ROC curves from different models", outer = TRUE)
```
## Model performance based on different metrics ( AUC, Accuracy, Sensitivity[Recall])

```{r}

# auc result
nnetAuc <- auc(roc_nnet)
marsAuc <- auc(roc_mars)
svmAuc <- auc(roc_svm)
svmpAuc <- auc(roc_svmp)
knnAuc <- auc(roc_knn)
rfAuc <- auc(roc_rf)
lrAuc <- auc(roc_lr)


# accuracy result
nnetAcc <- get_accuracy(nnet_model, test_X, test_y)
marsAcc <- get_accuracy(mars_model, test_X, test_y)
svmAcc <- get_accuracy(svm_model, test_X, test_y)
svmpAcc <- get_accuracy(svm_modelPoly, test_X, test_y)
knnAcc <- get_accuracy(knn_model, test_X, test_y)
rfAcc <- get_accuracy(rf_model, test_X, test_y)
lrAcc <- get_accuracy(lr_model, test_X, test_y)


auc_df <- data.frame(
  Model = c("Neural Network", "MARS", "Support Vector Machine (svmRadial)", "Support Vector Machine (svmPoly)",
                               "K-Nearest Neighbor", "Random Forest", "Logistic Regression"),
  
  AUC = c(nnetAuc, marsAuc, svmAuc, svmpAuc, knnAuc, rfAuc, lrAuc), 
  Accuracy = c(nnetAcc, marsAcc, svmAcc, svmpAcc, knnAcc, rfAcc, lrAcc)
)

print(auc_df)

# best model based on the AUC curve 
best_model <- auc_df[which.max(auc_df$AUC), ]

print(best_model)

```

## Checking the important variables of the optimal model 

```{r}
plot(varImp(rf_model, scale = FALSE), top = 10,
     main = "Important Factors for Predicting Cancer using Random Forest")

```






